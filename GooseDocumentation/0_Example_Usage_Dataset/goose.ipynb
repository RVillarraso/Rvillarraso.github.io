{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os, csv, glob\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from typing import Dict, Iterable, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GOOSE Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datadict from folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __check_labels(img_path: str, lbl_path: str) -> bool:\n",
    "    '''\n",
    "    Check if pair of labels and images exist\n",
    "    '''\n",
    "    name = os.path.basename(img_path)\n",
    "    name, ext = name.split('.')\n",
    "    name = name.split('_')[:-2]\n",
    "    name = '_'.join(name)\n",
    "    \n",
    "    names = []\n",
    "    for l in ['color', 'instanceids', 'labelids']:\n",
    "        # Check if label exists\n",
    "        lbl_name = name + '_' + l + '.' + ext\n",
    "        if not os.path.exists(os.path.join(lbl_path, lbl_name)):\n",
    "            return False, None\n",
    "        names.append(lbl_name)\n",
    "\n",
    "    return True, names\n",
    "\n",
    "def __goose_datadict_folder(img_path: str, lbl_path: str):\n",
    "    '''\n",
    "    Create a data Dict with image paths\n",
    "    '''\n",
    "    subfolders = glob.glob(os.path.join(img_path, '*/'), recursive = False)\n",
    "    subfolders = [f.split('/')[-2] for f in subfolders]\n",
    "\n",
    "    valid_imgs = []\n",
    "    valid_lbls = []\n",
    "    valid_insta= []\n",
    "    valid_color= []\n",
    "\n",
    "    datadict = []\n",
    "\n",
    "    for s in tqdm.tqdm(subfolders):\n",
    "        imgs_p  = os.path.join(img_path, s)\n",
    "        lbls_p  = os.path.join(lbl_path, s)\n",
    "        imgs    = glob.glob(os.path.join(imgs_p, '*.png'))\n",
    "        \n",
    "        for i in imgs:\n",
    "            valid, lbl_names = __check_labels(i, lbls_p)\n",
    "            if not valid:\n",
    "                continue\n",
    "\n",
    "            valid_imgs.append(i)\n",
    "            valid_color.append(os.path.join(lbls_p, lbl_names[0]))\n",
    "            valid_insta.append(os.path.join(lbls_p, lbl_names[1]))\n",
    "            valid_lbls.append(os.path.join(lbls_p,  lbl_names[2]))\n",
    "\n",
    "    for i,m,p,c in zip(valid_imgs, valid_lbls, valid_insta, valid_color):\n",
    "        datadict.append({\n",
    "                'img_path': i,\n",
    "                'semantic_path': m,\n",
    "                'instance_path':p,\n",
    "                'color_path': c,\n",
    "            })   \n",
    "\n",
    "    return datadict\n",
    "\n",
    "def goose_create_dataDict(src_path: str, mapping_csv_name: str = 'goose_label_mapping.csv') -> Dict:\n",
    "    '''\n",
    "    Parameters:\n",
    "\n",
    "        src_path            :   path to dataset\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        datadict_train      : dict with the dataset train images information\n",
    "        \n",
    "        datadict_val        : dict with the dataset validation images information\n",
    "        \n",
    "        datadict_test       : dict with the dataset test images information\n",
    "    '''\n",
    "    if mapping_csv_name is not None:\n",
    "        mapping_path = os.path.join(src_path, mapping_csv_name)\n",
    "        mapping = []\n",
    "        with open(mapping_path, newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for r in reader:\n",
    "                mapping.append(r)\n",
    "    else:\n",
    "        mapping = None\n",
    "\n",
    "    img_path = os.path.join(src_path, 'images')\n",
    "    lbl_path = os.path.join(src_path, 'labels')\n",
    "\n",
    "    datadicts = []\n",
    "    for c in ['test', 'train', 'val']:\n",
    "        print(\"### \" + c.capitalize() + \" Data ###\")\n",
    "        datadicts.append(\n",
    "            __goose_datadict_folder(\n",
    "                os.path.join(img_path, c),\n",
    "                os.path.join(lbl_path, c)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    test,train,val = datadicts\n",
    "\n",
    "    return test,train,val, mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Module for Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GOOSE_SemanticDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Example Pytorch Dataset Module for semantic tasks with GOOSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dict: List[Dict], crop: bool = True, resize_size: Iterable[int] = None):\n",
    "        '''\n",
    "        Parameters:\n",
    "            dataset_dict  [Iter]    : List of  Dicts with the images information generated by *goose_create_dataDict*\n",
    "            \n",
    "            crop          [Bool]    : Whether to make a square crop of the images or not\n",
    "            \n",
    "            resize_size   [Iter]    : List with the target resize size of the images (After the crop if crop == True)\n",
    "        '''\n",
    "        self.dataset_dict   = dataset_dict\n",
    "        self.transforms     = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "        self.resize_size    = resize_size\n",
    "        self.crop           = crop\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        if image is None:\n",
    "            return None\n",
    "        \n",
    "        if self.crop:\n",
    "            # Square-Crop in the center\n",
    "            s = min([image.width , image.height])\n",
    "            image = transforms.CenterCrop((s,s)).forward(image)\n",
    "\n",
    "        if self.resize_size is not None:\n",
    "            # Resize to given size\n",
    "            image       = image.resize(self.resize_size, resample=Image.NEAREST)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        '''\n",
    "        Parameter:\n",
    "            i   [int]                   : Index of the image to get\n",
    "\n",
    "        Returns:\n",
    "            image_tensor [torch.Tensor] : 3 x H x W Tensor\n",
    "            \n",
    "            label_tensor [torch.Tensor] : H x W Tensor as semantic map\n",
    "        '''\n",
    "        image       = Image.open(self.dataset_dict[i]['img_path']).convert('RGB')\n",
    "        label       = Image.open(self.dataset_dict[i]['semantic_path']).convert('L')\n",
    "\n",
    "        image       = self.preprocess(image)\n",
    "        label       = self.preprocess(label)\n",
    "\n",
    "        image_tensor = self.transforms(image)\n",
    "        label_tensor = torch.from_numpy(np.array(label)).long()\n",
    "\n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/path/to/goose'\n",
    "PATH = \"/home/miguel/Datasets/goose/goose2d/\"\n",
    "test_dict, train_dict, val_dict, mapping_dict = goose_create_dataDict(PATH)\n",
    "\n",
    "train_dataset = GOOSE_SemanticDataset(train_dict, crop=True, resize_size=(768,768))\n",
    "val_dataset   = GOOSE_SemanticDataset(val_dict, crop=True, resize_size=(768,768))\n",
    "test_dataset   = GOOSE_SemanticDataset(test_dict, crop=True, resize_size=(768,768))\n",
    "\n",
    "N_CLASSES = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Training\n",
    "\n",
    "In this example we use [*supergradients*](https://www.supergradients.com/) as a demo for training with GOOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import super_gradients as sg\n",
    "from super_gradients.training.metrics.segmentation_metrics import IoU\n",
    "from super_gradients.common.object_names import Models\n",
    "\n",
    "EXPERIMENT_NAME = \"demo\"\n",
    "WS_PATH = os.getcwd()\n",
    "CHECKPOINT_DIR = os.path.join(WS_PATH, 'output', 'ckpts')\n",
    "\n",
    "if not os.path.isdir(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "BATCH_SIZE      = 5\n",
    "VAL_BATCH_SIZE  = 5\n",
    "N_EPOCHS        = 10\n",
    "\n",
    "train_dataloader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=5, drop_last=True)\n",
    "val_dataloader      = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=5, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "sg.setup_device(device=device)\n",
    "trainer = sg.Trainer(experiment_name=EXPERIMENT_NAME, ckpt_root_dir=CHECKPOINT_DIR)\n",
    "\n",
    "model = sg.training.models.get(model_name=Models.DDRNET_39,\n",
    "                    num_classes=N_CLASSES,\n",
    "                    pretrained_weights='cityscapes')\n",
    "model.eval()\n",
    "\n",
    "lr_updates = [int(.3 * N_EPOCHS), int(.6 * N_EPOCHS), int(.9 * N_EPOCHS)]\n",
    "train_params = {\n",
    "            \"max_epochs\": N_EPOCHS,\n",
    "            \"lr_mode\":\"step\",\n",
    "            \"lr_updates\": lr_updates,\n",
    "            \"lr_decay_factor\": 0.1,\n",
    "            \"initial_lr\": 0.005,\n",
    "            \"optimizer\": 'sgd',\n",
    "            \"loss\": 'cross_entropy',\n",
    "            \"average_best_models\": False,\n",
    "            \"greater_metric_to_watch_is_better\": True,\n",
    "            \"loss_logging_items_names\": [\"loss\"],\n",
    "            \"drop_last\": True,\n",
    "            }\n",
    "\n",
    "train_params[\"train_metrics_list\"] = [IoU(num_classes=N_CLASSES)]\n",
    "train_params[\"valid_metrics_list\"] = [IoU(num_classes=N_CLASSES)]\n",
    "train_params[\"metric_to_watch\"]    = \"IoU\"\n",
    "\n",
    "trainer.train(model=model, training_params=train_params, train_loader=train_dataloader, valid_loader=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model\n",
    "model = model = sg.training.models.get(model_name=Models.DDRNET_39,\n",
    "                        num_classes=N_CLASSES,\n",
    "                        checkpoint_path=os.path.join(CHECKPOINT_DIR, EXPERIMENT_NAME, 'ckpt_best.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "N_SAMPLES = 10\n",
    "\n",
    "def run_inference(img, model):\n",
    "    if len(img.shape) != 4:\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "    mask = model(img)\n",
    "    masks = torch.sigmoid(mask).squeeze()\n",
    "    label = torch.max(masks, 0)[1]\n",
    "\n",
    "    return label\n",
    "\n",
    "model.eval()\n",
    "\n",
    "viridis = matplotlib.colormaps['viridis'].resampled(N_CLASSES)\n",
    "for idx in np.random.randint(0, len(test_dataset), min(N_SAMPLES, len(test_dataset))):\n",
    "        # Configure plot\n",
    "        plt.figure()\n",
    "        f, axarr = plt.subplots(1,3)\n",
    "        f.subplots_adjust(hspace=10.0, right=1.5)\n",
    "\n",
    "        axarr[0].set_xlabel(\"RGB\")\n",
    "        axarr[1].set_xlabel(\"Predicted\")\n",
    "        axarr[2].set_xlabel(\"Ground Truth\")\n",
    "\n",
    "        # Get images\n",
    "        img, label = test_dataset[idx]\n",
    "        mask = run_inference(img, model)\n",
    "\n",
    "        imgs = [np.transpose(img, (1, 2, 0)), np.asarray(mask), np.asarray(label)]\n",
    "\n",
    "        # Represent\n",
    "        for i in range(len(axarr)):\n",
    "            if i != 0:\n",
    "                im = axarr[i].imshow(imgs[i], cmap = viridis)\n",
    "                im.set_clim(0, N_CLASSES)\n",
    "            \n",
    "                # Legend\n",
    "                handles = []\n",
    "                for i_c,c in enumerate(np.unique(imgs[i])):\n",
    "                    segment_id = i_c\n",
    "                    segment_label = c\n",
    "                    label = f\"{segment_label}\"\n",
    "                    color = viridis(segment_label / 63)\n",
    "                    handles.append(mpatches.Patch(color=color, label=label))\n",
    "                axarr[i].legend(handles=handles, bbox_to_anchor=(1.0, 1.00))\n",
    "            else:\n",
    "                im = axarr[i].imshow(imgs[i])\n",
    "            \n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
