{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b356e63-b2b0-49f9-8d26-e2e7b598c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import super_gradients as sg\n",
    "from super_gradients.training.metrics.segmentation_metrics import IoU\n",
    "\n",
    "## Load the data\n",
    "#\n",
    "PATH = '/path/to/goose'\n",
    "test_dict, train_dict, val_dict, mapping_dict = goose_create_dataDict(PATH)\n",
    "\n",
    "train_dataset = GOOSE_SemanticDataset(train_dict, crop=True, resize_size=(768,768))\n",
    "val_dataset   = GOOSE_SemanticDataset(val_dict, crop=True, resize_size=(768,768))\n",
    "\n",
    "## Set-up for training\n",
    "#\n",
    "\n",
    "# Create output directory\n",
    "EXPERIMENT_NAME = \"GOOSE_train\"\n",
    "WS_PATH = os.getcwd()\n",
    "CHECKPOINT_DIR = os.path.join(WS_PATH, 'output', 'ckpts')\n",
    "\n",
    "if not os.path.isdir(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "# Params\n",
    "BATCH_SIZE      = 5\n",
    "N_EPOCHS        = 10\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=5, drop_last=True)\n",
    "val_dataloader      = DataLoader(val_dataset,  batch_size=BATCH_SIZE, shuffle=True, num_workers=5, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fac0b5-4386-495f-9392-57ac71d5c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Set-up\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "sg.setup_device(device=device)\n",
    "trainer = sg.Trainer(experiment_name=EXPERIMENT_NAME, ckpt_root_dir=CHECKPOINT_DIR)\n",
    "\n",
    "## Load Model\n",
    "#\n",
    "model = sg.training.models.get(model_name=Models.DDRNET_39,\n",
    "                    num_classes=64,\n",
    "                    pretrained_weights='cityscapes')\n",
    "model.eval()\n",
    "\n",
    "# Set-up Training params\n",
    "lr_updates = [int(.3 * N_EPOCHS), int(.6 * N_EPOCHS), int(.9 * N_EPOCHS)]\n",
    "train_params = {\n",
    "            \"max_epochs\": N_EPOCHS,\n",
    "            \"lr_mode\":\"step\",\n",
    "            \"lr_updates\": lr_updates,\n",
    "            \"lr_decay_factor\": 0.1,\n",
    "            \"initial_lr\": 0.005,\n",
    "            \"optimizer\": 'sgd',\n",
    "            \"loss\": 'cross_entropy',\n",
    "            \"average_best_models\": False,\n",
    "            \"greater_metric_to_watch_is_better\": True,\n",
    "            \"loss_logging_items_names\": [\"loss\"],\n",
    "            \"drop_last\": True,\n",
    "            }\n",
    "\n",
    "train_params[\"train_metrics_list\"] = [IoU(num_classes=64)]\n",
    "train_params[\"valid_metrics_list\"] = [IoU(num_classes=64)]\n",
    "train_params[\"metric_to_watch\"]    = \"IoU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39e62e-9c2c-488d-905c-c3a7f57f92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "trainer.train(model=model, training_params=train_params, train_loader=train_dataloader, valid_loader=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
